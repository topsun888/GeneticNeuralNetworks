{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Optimized by Quantum Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, feedforward neural networks are optimized by backpropagation method, to get the derivatives of weights and biases, and optimize them during each iteration of stochastic gradient descending. However, sometimes it is difficult to get the derivatives of the cost functions and sometimes the cost functions are not the same with the metrics. There are alternatives for backpropagation methods, such as genetic algorithms which can search for optimal parameters using iterations of populations and generations.\n",
    "\n",
    "The conventional genetic algorithm can generate a large population of specified neural networks with different parameters, each neural network with a specific parameter combinations is an individual. Then through calculation of fitnesses, cross-over, mutations, the algorithm tries to evolve the population in terms of the fitness.\n",
    "\n",
    "An important factor of applying genetic algorithm to real-world problem is encoding, namely encoding the key factors into genes, such as binary coding, namely encode a parameter as a series of 0s or 1s.  However, In this project, we are going to encode the weights and biases with quantums. The encoding method refers to Dr. Wang Zhiteng's [paper](https://wenku.baidu.com/view/8fe5bc385a8102d276a22f8b.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to validate the performances of Quantum Genetic ALgorithm and compare it to classical machine learning algorithms such as fully-connected neural networks, we use Iris dataset and MNIST to do training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Model\n",
    "\n",
    "MNIST is a very classical image dataset for classification, first we can build a neural network by sklearn, and the parameters will be updated by backpropogation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,), \n",
    "                    activation='logistic', verbose=0, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(20,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy 0.692\n"
     ]
    }
   ],
   "source": [
    "preds = mlp.predict(X_train)\n",
    "print('Training Accuracy', round(np.mean(preds == y_train), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy 0.567\n"
     ]
    }
   ],
   "source": [
    "preds = mlp.predict(X_test)\n",
    "print('Testing Accuracy', round(np.mean(preds == y_test), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 2, 1, 0, 0, 1, 2])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is pretty easy to design and optimize a neural network by tools like sklearn or tensorflow based on backpropagation method, but note, **the cost function must be differentiable in order to get gradients and the metric may not be the same with the cost function**. For example, we can use cross-entropy as our cost function for classification problems, however we often use accuracy as the metrics which is more explicit although not differentiable. \n",
    "\n",
    "Next, we are going to propose a quantum genetic algorithm, which does not rely on differentiable cost functions, you can define any cost functions. In addition, the mechanism of genetic algorithm can make use of distributed computing as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compute feedforward propagation given input and weights, it is quite easy to initialize them. And with different parameters, we can initialize different neural networks.\n",
    "## Qubit\n",
    "A qubit is a pair of $\\alpha=sin(\\theta)$ and $\\beta=cos(\\theta)$, which shows the uncertainties of a state.\n",
    "\n",
    "## Qubit Encoding\n",
    "In a quantum genetic algorithm, each chromosom consists of a series of qubits that forms a quantum registra. For example, suppose we have $m$ parameters, and each parameter can be encoded as a 5-qubit series, consequently the length of the chromosome should be $5m$. \n",
    "\n",
    "For example, we can denote the $ith$ parameter $x_i$:\n",
    "$$\\left(\\begin{array}{ccccc}\n",
    "\t\t\\alpha_{i1} & \\alpha_{i2} & \\alpha_{i3} & \\alpha_{i4} & \\alpha_{i5}\\\\ \n",
    "\t\t\\beta_{i1} & \\beta_{i2} & \\beta_{i3} & \\beta_{i4} & \\beta_{i5}\n",
    "\t\\end{array}\\right)$$\n",
    "    \n",
    " ## Collapsing\n",
    " We can map the qubits series above into states(5 binaries) according to certain criteria(will discuss later), for example we can suppose the collapsed states of the qubits above as:\n",
    " $$s_i = [0 \\ 1 \\ 1\\ 0 \\ 0]$$\n",
    " \n",
    " ## Value Calculating\n",
    " Now that we have the collapsed states of a parameter $x_i$, we can calculate the decimal value of it with a specified range:\n",
    " $$x_i = lowerbound + \\frac {\\sum_{j=1}^5 s_{ij} 2^{j-1}} {2^5-1} * (upperbound- lowerbound)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize weights and biases according to uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "#y_train = enc.fit_transform(y_train.reshape(-1, 1))\n",
    "#y_test = enc.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qubit_len = 10\n",
    "def initializeWeights(layers):\n",
    "    if type(layers) is not list:\n",
    "        print('Wrong input!')\n",
    "        return None\n",
    "    layers_num = len(layers)\n",
    "    weights = []\n",
    "    for l in range(layers_num-1):\n",
    "        #Generate angles\n",
    "        weight = {}\n",
    "        #Each weight varibale is encoded with 4 qubits\n",
    "        #With the 4 qubits, we can map them into a value\n",
    "        degrees = 2 * math.pi * np.random.uniform(0, 1, size=(layers[l], layers[l+1], qubit_len))\n",
    "        weight['sin'] = np.sin(degrees)\n",
    "        weight['cos'] = np.cos(degrees)\n",
    "        weight['degree'] = degrees\n",
    "        weights.append(weight)\n",
    "    return weights\n",
    "\n",
    "def initializeBiases(layers):\n",
    "    if type(layers) is not list:\n",
    "        print('Wrong input!')\n",
    "        return None\n",
    "    layers_num = len(layers)\n",
    "    biases = []\n",
    "    for l in range(layers_num-1):\n",
    "        bias = {}\n",
    "        #Each bias is encoded in 4 qubits\n",
    "        #With the 4 qubits, we can map them into a value\n",
    "        degrees = 2 * math.pi * np.random.uniform(0, 1, size=(layers[l+1], qubit_len))\n",
    "        bias['sin'] = np.sin(degrees)\n",
    "        bias['cos'] = np.cos(degrees)\n",
    "        bias['degrees'] = degrees\n",
    "        biases.append(bias)\n",
    "    return biases\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#layers = [784, 32, 10]\n",
    "layers = [4, 20, 3]\n",
    "weights_qubit = initializeWeights(layers)\n",
    "biases_qubit = initializeBiases(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class feedforwardnetwork:\n",
    "    '''\n",
    "    Args:\n",
    "    input_data: input matrix,[batch_size X feature_num], array\n",
    "    input_labels: one-hot labels, array\n",
    "    weights_qubit: define the weights, qubit encoding, list\n",
    "    biases_qubit: define the biases, qubit_encoding, list\n",
    "    '''\n",
    "    def __init__(self, input_data, input_labels, weights_qubit, biases_qubit):\n",
    "        self.input_data = input_data\n",
    "        self.input_labels = input_labels\n",
    "        self.weights_qubit = weights_qubit\n",
    "        self.biases_qubit = biases_qubit\n",
    "        \n",
    "    #Define sigmoid function\n",
    "    def sigmoid(self, Z):\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "    \n",
    "    #Calculate the accuracy\n",
    "    def calAccuracy(self, y, y_test):\n",
    "        '''Calculate Accuracy'''\n",
    "        return np.mean(y==y_test)\n",
    "    \n",
    "    def collapse(self, alpha, beta):\n",
    "        '''\n",
    "        Collapse the quantum state into a binary value\n",
    "        To calculate values later\n",
    "        Args:\n",
    "        alpha: the first state\n",
    "        beta: the second state\n",
    "        '''\n",
    "        pick = np.random.uniform(0, 1, alpha.shape)\n",
    "        #If the random value greater than alpha square, the return true\n",
    "        #Note this can lead to uncertainties for the results\n",
    "        states = np.where(pick > alpha**2, 1, 0)\n",
    "        return states\n",
    "\n",
    "    \n",
    "    def binary2decimal(self, states, bound):\n",
    "        '''\n",
    "        Map binary values of a variable into a decimal value\n",
    "        Each parameter can be mapped into binary bits\n",
    "        Args:\n",
    "        states: binary bits, like [0 1 1 0]\n",
    "        bound: the border of the variable\n",
    "        \n",
    "        Return: variable values\n",
    "        '''\n",
    "        shape = states.shape\n",
    "        #For weights\n",
    "        if len(shape) > 2:\n",
    "            qubit_num = shape[-1]\n",
    "            values = np.zeros((shape[0], shape[1]))\n",
    "            for l in np.arange(qubit_num):\n",
    "                values += states[:, :, l] * (2**l)\n",
    "                values = -bound + values/(2**qubit_len-1)*2*bound\n",
    "        #For biases\n",
    "        else:\n",
    "            qubit_num = shape[-1]\n",
    "            values = np.zeros((shape[0]))\n",
    "            for l in np.arange(qubit_num):\n",
    "                values += states[:, l] * (2**l)\n",
    "                values = -bound + values/(2**qubit_len-1)*2*bound\n",
    "        return values\n",
    "    \n",
    "    def quantum2value(self):\n",
    "        '''\n",
    "        Map varible qubits into decimal values\n",
    "        '''\n",
    "        self.weights = []\n",
    "        self.weights_bits = []\n",
    "        for weight_qubit in self.weights_qubit:\n",
    "            states = self.collapse(weight_qubit['sin'], weight_qubit['cos'])\n",
    "            values = self.binary2decimal(states, 0.5)\n",
    "            self.weights.append(values)\n",
    "            self.weights_bits.append(states)\n",
    "        self.biases = []\n",
    "        self.biases_bits = []\n",
    "        for bias_qubit in self.biases_qubit:\n",
    "            states = self.collapse(bias_qubit['sin'], bias_qubit['cos'])\n",
    "            values = self.binary2decimal(states, 0.5)\n",
    "            self.biases.append(values)\n",
    "            self.biases_bits.append(states)\n",
    "        #return self.weights, self.biases\n",
    "        \n",
    "    #Create a function to do predictions\n",
    "    #Map a one-hot vector to a number\n",
    "    def vec2num(self, data, label_num=10):\n",
    "        '''Make predictions'''\n",
    "        if len(data.shape) < 2:\n",
    "            print('The input has too few dimensions')\n",
    "            return None\n",
    "        #Select the class which has largest probability\n",
    "        #predictions = [(z.argmax()+ 1)%label_num for z in data]\n",
    "        predictions = data.argmax(axis=1)\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    \n",
    "    def costFuncWithReg(self, h, lambda1=0.01):\n",
    "        '''\n",
    "        Calculate the cost of neural network\n",
    "        Note here we use cross entropy\n",
    "        Regularization is also taken into account\n",
    "        '''\n",
    "        y = self.input_labels\n",
    "        if h is None or y is None or self.weights is None:\n",
    "            print('Invalid Input!')\n",
    "            return None\n",
    "        sample_num = len(y)#Length of y\n",
    "        #Cost of errors\n",
    "        total = -np.mean(np.sum(y*np.log(h), axis=1))\n",
    "        #Cost of regularization\n",
    "        weights = np.array(self.weights)\n",
    "        reg = 0\n",
    "        for wgt in weights:\n",
    "            reg += np.sum(wgt**2) * lambda1/2/sample_num\n",
    "        total +=  reg    \n",
    "        return total \n",
    "    \n",
    "    def proceed(self):\n",
    "        '''\n",
    "        Finish the procedure of feed forward network \n",
    "        and calculate the output\n",
    "        '''\n",
    "        self.quantum2value()\n",
    "        h, output_h, input_z = self.feedforwardNeuralNetwork(self.input_data)\n",
    "        #labels = self.vec2num(self.input_labels)\n",
    "        labels = self.input_labels\n",
    "        predictions = self.vec2num(h)\n",
    "        #self.fitness = self.costFuncWithReg(h)\n",
    "        self.accuracy = self.calAccuracy(labels, predictions)\n",
    "        self.fitness = self.accuracy\n",
    "        \n",
    "    def updateWeightBias(self, new_weights_qubit, new_biases_qubit):\n",
    "        '''\n",
    "        Update weights and biases\n",
    "        '''\n",
    "        self.weights_qubit = new_weights_qubit\n",
    "        self.biases_qubit = new_biases_qubit\n",
    "        \n",
    "    def evaluateTestData(self, new_input_data, new_input_labels):\n",
    "        '''\n",
    "        Update data\n",
    "        '''\n",
    "        X, y = new_input_data, new_input_labels\n",
    "        h, output_h, input_z = self.feedforwardNeuralNetwork(X)\n",
    "        labels = y\n",
    "        predictions = self.vec2num(h)\n",
    "        accuracy = self.calAccuracy(labels, predictions)\n",
    "        return accuracy\n",
    "    \n",
    "    #weights: weights for each layer, a list\n",
    "    def feedforwardNeuralNetwork(self, X):\n",
    "        '''Calculate feedforward propagation output'''\n",
    "        ######Deal with extreme cases###\n",
    "        if X is None or self.weights is None:\n",
    "            print('Invalid Input!')\n",
    "            return None\n",
    "        dim = X.shape\n",
    "        if len(dim) < 2:\n",
    "            print('X has too less variables')\n",
    "            return None\n",
    "\n",
    "        #####Define variables###########\n",
    "        layer_num = len(self.weights)\n",
    "        output_h = []#Output for each layer\n",
    "        output_h.append(X)#The first layer is equal to input X\n",
    "        input_z = []#Input for each layer, starts from the second layer\n",
    "        #####Make alculations for each layer, except the input layer\n",
    "        for i in range(layer_num):\n",
    "            z = np.dot(output_h[i], self.weights[i])\n",
    "            z += self.biases[i]\n",
    "            h = self.sigmoid(z)\n",
    "            output_h.append(h)\n",
    "            input_z.append(z)\n",
    "        return h, output_h, input_z    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train = mnist.train.images, mnist.train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fw = feedforwardnetwork(X_train, y_train, weights_qubit, biases_qubit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing:0.002\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "fw.proceed()\n",
    "end = time()\n",
    "print('Timing:{:.3f}'.format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes much time to finish computation for each individual. So it must be time-consuming if we have a large population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw.fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fw.updateWeightBias(weights_qubit, biases_qubit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fw.proceed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36666666666666664"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw.fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20000000000000001"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw.evaluateTestData(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an feedforward object, we can create an instance of neural network by specifying the parameters. We can also create a population of neural networks with different parameters. How to generate the parameters, that's a question we will answer by genetic algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Create population\n",
    "\n",
    "In this project, we can view the parameters of neural networks as genes, our goal is to find optimal genes, namely   parameters that make those neural networks performance well on predictions.\n",
    "There are four major steps in a genetic algorithm:\n",
    "- Create Population, randomly create parameters and generate neural networks according to the parameters\n",
    "- Calculate fitness, calculate the fitness of each individual neural network\n",
    "- Quantum rotating gates, compared to conventional genetic algorithm, QGA uses quantum rotating gates to update chromosomes.\n",
    "- Mutate, select certain individual neural networks and make mutations on their chromosomes in terms of NOT gating.\n",
    "\n",
    "For more information you can read the paper The Improvement of Quantum Genetic Algorithm and Its Application on Function Optimization, written by Huaixiao Wang, Jianyong Liu and etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create a group of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class initPopulation:\n",
    "    def __init__(self, layers, pop_size=1000):\n",
    "        '''\n",
    "        Args:\n",
    "        pop_size: number of population, integer\n",
    "        layers: neuron number for each layer, list\n",
    "        '''\n",
    "        self.pop_size = pop_size\n",
    "        self.layers = layers\n",
    "        \n",
    "    def __initializeWeights__(self):\n",
    "        '''\n",
    "        Initialize weights between each two neigbouring layers\n",
    "        '''\n",
    "        layers = self.layers\n",
    "        if type(layers) is not list:\n",
    "            print('Wrong input!')\n",
    "            return None\n",
    "        layers_num = len(layers)\n",
    "        weights = []\n",
    "        for l in range(layers_num-1):\n",
    "            #Generate angles\n",
    "            weight = {}\n",
    "            #Each weight varibale is encoded with 4 qubits\n",
    "            #With the 4 qubits, we can map them into a value\n",
    "            #degrees = 0.25 * np.pi * np.ones((layers[l], layers[l+1], qubit_len))\n",
    "            degrees = np.ones((layers[l], layers[l+1], qubit_len)) * math.pi/4\n",
    "            weight['sin'] = np.sin(degrees)\n",
    "            weight['cos'] = np.cos(degrees)\n",
    "            weight['degree'] = degrees\n",
    "            weights.append(weight)\n",
    "        return weights\n",
    "\n",
    "    def __initializeBiases__(self):\n",
    "        '''\n",
    "        Initialize biases for each layer(except for input layer)\n",
    "        '''\n",
    "        layers = self.layers\n",
    "        if type(layers) is not list:\n",
    "            print('Wrong input!')\n",
    "            return None\n",
    "        layers_num = len(layers)\n",
    "        biases = []\n",
    "        for l in range(layers_num-1):\n",
    "            bias = {}\n",
    "            #Each bias is encoded in 4 qubits\n",
    "            #With the 4 qubits, we can map them into a value\n",
    "            #degrees = 0.25 * np.pi * np.ones((layers[l], layers[l+1], qubit_len))\n",
    "            degrees = np.ones((layers[l+1], qubit_len)) * math.pi/4\n",
    "            bias['sin'] = np.sin(degrees)\n",
    "            bias['cos'] = np.cos(degrees)\n",
    "            bias['degree'] = degrees\n",
    "            biases.append(bias)\n",
    "        return biases\n",
    "    \n",
    "    def generatePop(self):\n",
    "        '''\n",
    "        Generate a group of weights at random\n",
    "        '''\n",
    "        population = []\n",
    "        for i in range(self.pop_size):\n",
    "            weights = self.__initializeWeights__()\n",
    "            biases = self.__initializeBiases__()\n",
    "            #Initialize an individual\n",
    "            individual = feedforwardnetwork(X_train, y_train, weights, biases)\n",
    "            #Calculate fitness\n",
    "            individual.proceed()\n",
    "            population.append(individual)\n",
    "        return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pop = initPopulation(layers)\n",
    "population = pop.generatePop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3.Genetic Algorithm\n",
    "\n",
    "In this part, we are going to realize quantum rotation gates, mutation. \n",
    "## Quantum Rotation Gates\n",
    "\n",
    "Quantum rotation gates is a method to change the probabilities amplitube by shifting the angles which updates the states of parameters, and eventually uodates the values of parameters. We can show the process in a formula below.\n",
    "Note, we introduce the encoding procedurings above, $x_i$ can be encoded in a qubit string with specific lengths:\n",
    "$$\\left(\\begin{array}{ccccc}\n",
    "\t\t\\alpha_{i1} & \\alpha_{i2} & \\alpha_{i3} & \\alpha_{i4} & \\alpha_{i5}\\\\ \n",
    "\t\t\\beta_{i1} & \\beta_{i2} & \\beta_{i3} & \\beta_{i4} & \\beta_{i5}\n",
    "\t\\end{array}\\right)$$\n",
    "    \n",
    " Suppose the $ith$ original qubit of parameter $x_i$ is $\\alpha_{ij}$ and $\\beta_{ij}$,\n",
    " $$\\alpha_{ij} = sin(\\theta_{ij}), \\ \\beta_{ij} = cos(\\theta_{ij})$$\n",
    " \n",
    " And the shift angle is $\\Delta \\theta_{ij}$, therefore the updated qubit of parameter $x_i$:\n",
    " $$\\hat \\alpha_{ij} = sin(\\theta_{ij}-\\Delta \\theta_{ij}) = sin(\\theta_{ij})cos(\\Delta \\theta_{ij})-cos(\\theta_{ij})sin(\\Delta \\theta_{ij})$$\n",
    "  $$\\hat \\beta_{ij} = cos(\\theta_{ij}-\\Delta \\theta_{ij}) = cos(\\theta_{ij})cos(\\Delta \\theta_{ij})+sin(\\theta_{ij})sin(\\Delta \\theta_{ij})$$\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can transform the formula into a matrix operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\left[\\begin{array}{ccccc}\n",
    "\t\t\\hat \\alpha_{ij} \\\\ \n",
    "\t\t\\hat \\beta_{ij}\n",
    "\t\\end{array}\\right] = \\left[\\begin{array}{ccccc}\n",
    "\t\tsin(\\Delta \\theta_{ij}) & -cos(\\Delta \\theta_{ij})\\\\ \n",
    "\t\tcos(\\Delta \\theta_{ij}) & sin(\\Delta \\theta_{ij}) \n",
    "\t\\end{array}\\right] \\left[\\begin{array}{ccccc}\n",
    "\t\t\\alpha_{ij} \\\\ \n",
    "\t\t\\beta_{ij}\n",
    "\t\\end{array}\\right]$$\n",
    "\n",
    "There are some strategies for rotation gates, here we adopt the one in Huaixiao Wang's paper as below:\n",
    "![rotation strategy](rotationGateStrategy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import  copy\n",
    "class GA:\n",
    "    def __init__(self, population):\n",
    "        '''\n",
    "        Initialize genetic algorithm\n",
    "        '''\n",
    "        self.population = copy.deepcopy(population)\n",
    "        self.pop_size = len(population)\n",
    "        \n",
    "    \n",
    "    def rotationMatrix(self, sgn, delta):\n",
    "        '''\n",
    "        Calculation the matrix of rotation gate\n",
    "        Args:\n",
    "        sgn: sign of the angle rotation direction, +1 or -1\n",
    "        delta: shift angle of the rotation gate\n",
    "        '''\n",
    "        e = sgn *delta\n",
    "        U = np.array([[np.cos(e), -np.sin(e)], [np.sin(e), np.cos(e)]])\n",
    "        return U\n",
    "    \n",
    "    def rotationAngleDirection(self, bestIndividual, obj):\n",
    "        '''\n",
    "        Calculate rotation angles and directions for each individual\n",
    "        '''\n",
    "        #Initialize the shift angle\n",
    "        delta_theta = 0.01 * math.pi\n",
    "        #Compare the fitness\n",
    "        fitness_flag = obj.fitness > bestIndividual.fitness\n",
    "        #Traverse each weight layer\n",
    "        for j in np.arange(len(obj.weights_qubit)):\n",
    "            #The jth layer\n",
    "            #Traverse each parameter in this layer\n",
    "            #qubit contains alpha and beta\n",
    "            #Alpha size: layer(i), layer(i+1), bit length\n",
    "            #A qubit is a pair of alpha and beta\n",
    "            weight_alpha = obj.weights_qubit[j]['sin']\n",
    "            #beta size: layer(i), layer(i+1), bit length\n",
    "            weight_beta = obj.weights_qubit[j]['cos']\n",
    "            #Original angles:\n",
    "            degrees = obj.weights_qubit[j]['degree']\n",
    "            #state size: layer(i), layer(i+1), bit length\n",
    "            #state, e.g [0 1 1 0]\n",
    "            weight_bit = obj.weights_bits[j]\n",
    "            #Traverse each parameter\n",
    "            best_weight_bit = bestIndividual.weights_bits[j]\n",
    "            #Search the table\n",
    "            criteria = (weight_bit + best_weight_bit) == 1\n",
    "            #Calculate shift angles for each parameter\n",
    "            delta = criteria * delta_theta\n",
    "            #Calculate the sign of shift angles\n",
    "            sgns = np.zeros(weight_bit.shape)#Initialize it with zeros\n",
    "            #Try to avoid loops, use matrix operation as much as mossible\n",
    "            #IF xi=0 besti=1, then the difference will be -1\n",
    "            current_best_bit_flag = weight_bit - best_weight_bit\n",
    "            #Create a matrix of fitness flag with the same shape as the weight\n",
    "            fitness_flags = np.ones(weight_bit.shape) * fitness_flag\n",
    "            #Map 0 into -1\n",
    "            fitness_flags = np.where(fitness_flags>0, 1, -1)\n",
    "            alpha_beta_pos = (weight_alpha * weight_beta) > 0\n",
    "            alpha_beta_neg = (weight_alpha * weight_beta) < 0\n",
    "            alpha_zero = weight_alpha == 0\n",
    "            beta_zero = weight_beta == 0\n",
    "            #if alpha * beta>0\n",
    "            sgns += current_best_bit_flag * fitness_flags * alpha_beta_pos\n",
    "            #if alpha * beta<0\n",
    "            sgns += (-1)*current_best_bit_flag * fitness_flags * alpha_beta_neg\n",
    "            #if alpha = 0\n",
    "            #Gnerate +1 -1 at random\n",
    "            direction = np.random.choice([1, -1], size=weight_bit.shape)\n",
    "            criteria = current_best_bit_flag * fitness_flags * alpha_zero < 0\n",
    "            sgns += criteria * direction\n",
    "            #if beta = 0\n",
    "            criteria = current_best_bit_flag * fitness_flags * beta_zero > 0\n",
    "            sgns += criteria * direction\n",
    "            #Calculate shift angles\n",
    "            angles = delta * sgns\n",
    "            #Calculate new angles\n",
    "            degrees = degrees - angles\n",
    "            obj.weights_qubit[j]['sin'] = np.sin(degrees)\n",
    "            obj.weights_qubit[j]['cos'] = np.cos(degrees)\n",
    "            obj.weights_qubit[j]['degree'] = degrees\n",
    "        #Update bias\n",
    "        for j in np.arange(len(obj.biases_qubit)):\n",
    "            #The jth bias\n",
    "            #Traverse each parameter in this layer\n",
    "            #qubit contains alpha and beta\n",
    "            #Alpha size: layer(i) bit length\n",
    "            #A qubit is a pair of alpha and beta\n",
    "            bias_alpha = obj.biases_qubit[j]['sin']\n",
    "            #beta size: layer(i), layer(i+1), bit length\n",
    "            bias_beta = obj.biases_qubit[j]['cos']\n",
    "            #Original angles:\n",
    "            degrees = obj.biases_qubit[j]['degree']\n",
    "            #state size: layer(i), layer(i+1), bit length\n",
    "            #state, e.g [0 1 1 0]\n",
    "            bias_bit = obj.biases_bits[j]\n",
    "            #Traverse each parameter\n",
    "            best_bias_bit = bestIndividual.biases_bits[j]\n",
    "            #Search the table\n",
    "            criteria = (bias_bit + best_bias_bit) == 1\n",
    "            #Calculate shift angles for each parameter\n",
    "            delta = criteria * delta_theta\n",
    "            #Calculate the sign of shift angles\n",
    "            #Shape: bias number * bit length\n",
    "            sgns = np.zeros(bias_bit.shape)#Initialize it with zeros\n",
    "            #Try to avoid loops, use matrix operation as much as mossible\n",
    "            #IF xi=0 besti=1, then the difference will be -1\n",
    "            current_best_bit_flag = bias_bit - best_bias_bit\n",
    "            #Create a matrix of fitness flag with the same shape as the weight\n",
    "            fitness_flags = np.ones(bias_bit.shape) * fitness_flag\n",
    "            #Map 0 into -1\n",
    "            fitness_flags = np.where(fitness_flags>0, 1, -1)\n",
    "            alpha_beta_pos = (bias_alpha * bias_beta) > 0\n",
    "            alpha_beta_neg = (bias_alpha * bias_beta) < 0\n",
    "            alpha_zero = bias_alpha == 0\n",
    "            beta_zero = bias_beta == 0\n",
    "            #if alpha * beta>0\n",
    "            sgns += current_best_bit_flag * fitness_flags * alpha_beta_pos\n",
    "            #if alpha * beta<0\n",
    "            sgns += (-1)*current_best_bit_flag * fitness_flags * alpha_beta_neg\n",
    "            #if alpha = 0\n",
    "            #Gnerate +1 -1 at random\n",
    "            direction = np.random.choice([1, -1], size=bias_bit.shape)\n",
    "            criteria = current_best_bit_flag * fitness_flags * alpha_zero < 0\n",
    "            sgns += criteria * direction\n",
    "            #if beta = 0\n",
    "            criteria = current_best_bit_flag * fitness_flags * beta_zero > 0\n",
    "            sgns += criteria * direction\n",
    "            #Calculate shift angles\n",
    "            angles = delta * sgns\n",
    "            #Calculate new angles\n",
    "            degrees = degrees - angles\n",
    "            obj.biases_qubit[j]['sin'] = np.sin(degrees)\n",
    "            obj.biases_qubit[j]['cos'] = np.cos(degrees)\n",
    "            obj.biases_qubit[j]['degree'] = degrees\n",
    "        obj.proceed()\n",
    "        return obj\n",
    "    \n",
    "    def rotatingGates(self, bestIndividual_index):\n",
    "        '''\n",
    "        Rotate gates of quantum registra,\n",
    "        Note, we try to make use of numpy's matrix operations to speed\n",
    "        Computation\n",
    "        Args:\n",
    "        bestIndividual_index: the index of the best individual\n",
    "        '''\n",
    "        bestIndividual = self.population[bestIndividual_index]\n",
    "        indices = [bestIndividual] * self.pop_size\n",
    "        #self.population = list(map(self.rotationAngleDirection, indices, self.population))\n",
    "        #Traverse each individual\n",
    "        for i in np.arange(self.pop_size):\n",
    "            obj = self.population[i]\n",
    "            obj = self.rotationAngleDirection(bestIndividual, obj)\n",
    "            self.population[i] = obj\n",
    "            \n",
    "    def NotGates(self, ratio=0.05):\n",
    "        '''\n",
    "        Rotate gates of quantum registra,\n",
    "        Note, we try to make use of numpy's matrix operations to speed\n",
    "        Computation\n",
    "        Args:\n",
    "        bestIndividual_index: the index of the best individual\n",
    "        '''\n",
    "        #Traverse each individual\n",
    "        num = int(self.pop_size * ratio)\n",
    "        indice = np.random.choice(self.pop_size, num)\n",
    "        for i in indice:\n",
    "            obj = self.population[i]\n",
    "            obj = self.mutation(obj)\n",
    "            self.population[i] = obj\n",
    "    \n",
    "    def mutation(self, obj):\n",
    "        '''\n",
    "        Mutation at several random point within an individual\n",
    "        '''\n",
    "        #Traverse each weight layer\n",
    "        for j in np.arange(len(obj.weights_qubit)):\n",
    "            #The jth layer\n",
    "            #Traverse each parameter in this layer\n",
    "            #qubit contains alpha and beta\n",
    "            #Alpha size: layer(i), layer(i+1), bit length\n",
    "            #A qubit is a pair of alpha and beta\n",
    "            weight_alpha = obj.weights_qubit[j]['sin']\n",
    "            #beta size: layer(i), layer(i+1), bit length\n",
    "            weight_beta = obj.weights_qubit[j]['cos']\n",
    "            #Degrees\n",
    "            degrees = obj.weights_qubit[j]['degree']\n",
    "            picks = np.random.uniform(0, 1, size=degrees.shape)\n",
    "            #state size: layer(i), layer(i+1), bit length\n",
    "            alpha_flag = weight_alpha < picks\n",
    "            beta_flag = weight_beta < picks\n",
    "            degrees = degrees - alpha_flag*beta_flag*math.pi/2\n",
    "            obj.weights_qubit[j]['sin'] = np.sin(degrees)\n",
    "            obj.weights_qubit[j]['cos'] = np.cos(degrees)\n",
    "            obj.weights_qubit[j]['degree'] = degrees\n",
    "        #Update bias\n",
    "        for j in np.arange(len(obj.biases_qubit)):\n",
    "            #The jth bias\n",
    "            #Traverse each parameter in this layer\n",
    "            #qubit contains alpha and beta\n",
    "            #Alpha size: layer(i) bit length\n",
    "            #A qubit is a pair of alpha and beta\n",
    "            bias_alpha = obj.biases_qubit[j]['sin']\n",
    "            #beta size: layer(i), layer(i+1), bit length\n",
    "            bias_beta = obj.biases_qubit[j]['cos']\n",
    "            #Original angles:\n",
    "            degrees = obj.biases_qubit[j]['degree']\n",
    "            picks = np.random.uniform(0, 1, size=degrees.shape)\n",
    "            alpha_flag = bias_alpha < picks\n",
    "            beta_flag = bias_beta < picks\n",
    "            degrees = degrees - alpha_flag*beta_flag*math.pi/2\n",
    "            #Calculate new angles\n",
    "            degrees = degrees - alpha_flag*beta_flag*math.pi/2\n",
    "            obj.biases_qubit[j]['sin'] = np.sin(degrees)\n",
    "            obj.biases_qubit[j]['cos'] = np.cos(degrees)\n",
    "            obj.biases_qubit[j]['degree'] = degrees\n",
    "        obj.proceed()\n",
    "        return obj\n",
    "        \n",
    "    def proceed(self, generation_num = 1):\n",
    "        '''\n",
    "        Execute quantum rotation and not gating\n",
    "        '''\n",
    "        #Keep the best individual\n",
    "        best_fitness, optimal_index, _ = self.findMaximalIndividual()\n",
    "        #Note the variables are referenced, we need to copy in a deep way\n",
    "        best_individual = copy.deepcopy(self.population[optimal_index])\n",
    "        for _ in np.arange(generation_num):\n",
    "            #Quantum Rotation Gate\n",
    "            print('Best Training Accuracy:', round(best_fitness, 4))\n",
    "            test_accuracy = best_individual.evaluateTestData(X_test, y_test)\n",
    "            print('Testing Accuracy:', round(test_accuracy, 4))\n",
    "            self.rotatingGates(optimal_index)\n",
    "            #Mutation\n",
    "            self.NotGates()\n",
    "            fitness, max_index, min_index = ga.findMaximalIndividual()\n",
    "            if fitness < best_fitness:\n",
    "                #Use deep copy or the two objects will be the same actually\n",
    "                self.population[max_index] = copy.deepcopy(best_individual)\n",
    "            else:\n",
    "                best_fitness = fitness\n",
    "                optimal_index = max_index\n",
    "                #Note deep copy here\n",
    "                best_individual = copy.deepcopy(self.population[optimal_index])\n",
    "        print('Final Training Accuracy:', round(best_fitness, 4))\n",
    "        return best_individual\n",
    "        \n",
    "    def findMaximalIndividual(self):\n",
    "        fitnesses = np.array([one.fitness for one in self.population])\n",
    "        optimal_value = max(fitnesses)\n",
    "        optimal_index = fitnesses.argmax()\n",
    "        minimal_index = fitnesses.argmin()\n",
    "        return optimal_value, optimal_index, minimal_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ga = GA(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training Accuracy: 0.825\n",
      "Testing Accuracy: 0.8333\n",
      "Best Training Accuracy: 0.825\n",
      "Testing Accuracy: 0.8333\n",
      "Best Training Accuracy: 0.825\n",
      "Testing Accuracy: 0.8333\n",
      "Best Training Accuracy: 0.825\n",
      "Testing Accuracy: 0.8333\n",
      "Best Training Accuracy: 0.825\n",
      "Testing Accuracy: 0.8333\n",
      "Best Training Accuracy: 0.825\n",
      "Testing Accuracy: 0.7333\n",
      "Best Training Accuracy: 0.825\n",
      "Testing Accuracy: 0.7333\n",
      "Best Training Accuracy: 0.825\n",
      "Testing Accuracy: 0.7333\n",
      "Best Training Accuracy: 0.825\n",
      "Testing Accuracy: 0.7333\n",
      "Best Training Accuracy: 0.825\n",
      "Testing Accuracy: 0.7333\n",
      "Best Training Accuracy: 0.825\n",
      "Testing Accuracy: 0.7333\n",
      "Best Training Accuracy: 0.825\n",
      "Testing Accuracy: 0.7333\n",
      "Best Training Accuracy: 0.9\n",
      "Testing Accuracy: 0.7667\n",
      "Best Training Accuracy: 0.9\n",
      "Testing Accuracy: 0.7667\n",
      "Best Training Accuracy: 0.9\n",
      "Testing Accuracy: 0.7667\n",
      "Best Training Accuracy: 0.9\n",
      "Testing Accuracy: 0.7667\n",
      "Best Training Accuracy: 0.9\n",
      "Testing Accuracy: 0.7667\n",
      "Best Training Accuracy: 0.9\n",
      "Testing Accuracy: 0.7667\n",
      "Best Training Accuracy: 0.9\n",
      "Testing Accuracy: 0.7667\n",
      "Best Training Accuracy: 0.9\n",
      "Testing Accuracy: 0.7667\n",
      "Best Training Accuracy: 0.9\n",
      "Testing Accuracy: 0.7667\n",
      "Best Training Accuracy: 0.9\n",
      "Testing Accuracy: 0.7667\n",
      "Best Training Accuracy: 0.9\n",
      "Testing Accuracy: 0.7667\n",
      "Best Training Accuracy: 0.9\n",
      "Testing Accuracy: 0.7667\n",
      "Best Training Accuracy: 0.9\n",
      "Testing Accuracy: 0.7667\n",
      "Best Training Accuracy: 0.9417\n",
      "Testing Accuracy: 0.9\n",
      "Best Training Accuracy: 0.9417\n",
      "Testing Accuracy: 0.9\n",
      "Best Training Accuracy: 0.9417\n",
      "Testing Accuracy: 0.9\n",
      "Best Training Accuracy: 0.9417\n",
      "Testing Accuracy: 0.9\n",
      "Best Training Accuracy: 0.9417\n",
      "Testing Accuracy: 0.9\n",
      "Best Training Accuracy: 0.9417\n",
      "Testing Accuracy: 0.9\n",
      "Best Training Accuracy: 0.9417\n",
      "Testing Accuracy: 0.9\n",
      "Best Training Accuracy: 0.9417\n",
      "Testing Accuracy: 0.9\n",
      "Best Training Accuracy: 0.9417\n",
      "Testing Accuracy: 0.9\n",
      "Best Training Accuracy: 0.9583\n",
      "Testing Accuracy: 0.9333\n",
      "Best Training Accuracy: 0.9583\n",
      "Testing Accuracy: 0.9333\n",
      "Best Training Accuracy: 0.9583\n",
      "Testing Accuracy: 0.9333\n",
      "Best Training Accuracy: 0.9583\n",
      "Testing Accuracy: 0.9333\n",
      "Best Training Accuracy: 0.9583\n",
      "Testing Accuracy: 0.9333\n",
      "Best Training Accuracy: 0.9583\n",
      "Testing Accuracy: 0.9333\n",
      "Best Training Accuracy: 0.9583\n",
      "Testing Accuracy: 0.9333\n",
      "Best Training Accuracy: 0.9583\n",
      "Testing Accuracy: 0.9333\n",
      "Best Training Accuracy: 0.9583\n",
      "Testing Accuracy: 0.9333\n",
      "Best Training Accuracy: 0.9583\n",
      "Testing Accuracy: 0.9333\n",
      "Best Training Accuracy: 0.9583\n",
      "Testing Accuracy: 0.9333\n",
      "Best Training Accuracy: 0.9583\n",
      "Testing Accuracy: 0.9333\n",
      "Best Training Accuracy: 0.9583\n",
      "Testing Accuracy: 0.9333\n",
      "Best Training Accuracy: 0.9583\n",
      "Testing Accuracy: 0.9333\n",
      "Best Training Accuracy: 0.9583\n",
      "Testing Accuracy: 0.9333\n",
      "Best Training Accuracy: 0.9583\n",
      "Testing Accuracy: 0.9333\n",
      "Final Training Accuracy: 0.9583\n"
     ]
    }
   ],
   "source": [
    "#Try 20 generations\n",
    "best = ga.proceed(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95833333333333337"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.evaluateTestData(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93333333333333335"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.evaluateTestData(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result is competitive compared to backpropagation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
