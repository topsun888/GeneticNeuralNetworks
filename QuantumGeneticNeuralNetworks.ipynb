{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Optimized by Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, feedforward neural networks are optimized by backpropagation method, to get the derivatives of weights and biases, and optimize them during each iteration of stochastic gradient descending. However, there are alternatives for backpropagation methods, such as genetic algorithms which can search for optimal parameters using iterations of populations and generations.\n",
    "\n",
    "The genetic algorithm can generate a large population of specified neural networks with different parameters, each neural network with a specific parameter combinations is an individual. Then through calculation of fitnesses, cross-over, mutations, the algorithm tries to evolve the population in terms of the fitness.\n",
    "\n",
    "An important factor of applying genetic algorithm to real-world problem is encoding, namely encoding the key factors into genes. In this project, we are going to encode the weights and biases with quantums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Model\n",
    "\n",
    "MNIST is a very classical image dataset for classification, first we can build a neural network by sklearn, and the parameters will be updated by backpropogation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(128,), \n",
    "                    activation='logistic', verbose=1, max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.82051515\n",
      "Iteration 2, loss = 0.32540352\n",
      "Iteration 3, loss = 0.25727768\n",
      "Iteration 4, loss = 0.21993796\n",
      "Iteration 5, loss = 0.19224606\n",
      "Iteration 6, loss = 0.17166673\n",
      "Iteration 7, loss = 0.15446920\n",
      "Iteration 8, loss = 0.13962322\n",
      "Iteration 9, loss = 0.12728169\n",
      "Iteration 10, loss = 0.11625108\n",
      "Iteration 11, loss = 0.10648947\n",
      "Iteration 12, loss = 0.09818904\n",
      "Iteration 13, loss = 0.09042377\n",
      "Iteration 14, loss = 0.08356566\n",
      "Iteration 15, loss = 0.07765001\n",
      "Iteration 16, loss = 0.07160196\n",
      "Iteration 17, loss = 0.06646980\n",
      "Iteration 18, loss = 0.06187605\n",
      "Iteration 19, loss = 0.05748994\n",
      "Iteration 20, loss = 0.05337661\n",
      "Iteration 21, loss = 0.04998204\n",
      "Iteration 22, loss = 0.04628259\n",
      "Iteration 23, loss = 0.04319991\n",
      "Iteration 24, loss = 0.04044016\n",
      "Iteration 25, loss = 0.03755347\n",
      "Iteration 26, loss = 0.03488591\n",
      "Iteration 27, loss = 0.03257383\n",
      "Iteration 28, loss = 0.03011273\n",
      "Iteration 29, loss = 0.02822235\n",
      "Iteration 30, loss = 0.02637966\n",
      "Iteration 31, loss = 0.02436130\n",
      "Iteration 32, loss = 0.02281195\n",
      "Iteration 33, loss = 0.02106009\n",
      "Iteration 34, loss = 0.01955060\n",
      "Iteration 35, loss = 0.01842153\n",
      "Iteration 36, loss = 0.01714866\n",
      "Iteration 37, loss = 0.01588998\n",
      "Iteration 38, loss = 0.01472087\n",
      "Iteration 39, loss = 0.01378746\n",
      "Iteration 40, loss = 0.01289348\n",
      "Iteration 41, loss = 0.01207002\n",
      "Iteration 42, loss = 0.01122144\n",
      "Iteration 43, loss = 0.01057745\n",
      "Iteration 44, loss = 0.00989120\n",
      "Iteration 45, loss = 0.00933175\n",
      "Iteration 46, loss = 0.00876683\n",
      "Iteration 47, loss = 0.00813072\n",
      "Iteration 48, loss = 0.00766536\n",
      "Iteration 49, loss = 0.00729544\n",
      "Iteration 50, loss = 0.00680951\n",
      "Iteration 51, loss = 0.00652735\n",
      "Iteration 52, loss = 0.00615383\n",
      "Iteration 53, loss = 0.00583949\n",
      "Iteration 54, loss = 0.00558042\n",
      "Iteration 55, loss = 0.00531104\n",
      "Iteration 56, loss = 0.00506869\n",
      "Iteration 57, loss = 0.00480330\n",
      "Iteration 58, loss = 0.00463622\n",
      "Iteration 59, loss = 0.00441441\n",
      "Iteration 60, loss = 0.00428215\n",
      "Iteration 61, loss = 0.00408576\n",
      "Iteration 62, loss = 0.00391944\n",
      "Iteration 63, loss = 0.00381637\n",
      "Iteration 64, loss = 0.00368788\n",
      "Iteration 65, loss = 0.00357172\n",
      "Iteration 66, loss = 0.00344735\n",
      "Iteration 67, loss = 0.00334131\n",
      "Iteration 68, loss = 0.00323329\n",
      "Iteration 69, loss = 0.00316301\n",
      "Iteration 70, loss = 0.00309633\n",
      "Iteration 71, loss = 0.00306378\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(128,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(mnist.train.images, mnist.train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "preds = mlp.predict(mnist.train.images)\n",
    "print('Training Accuracy', round(np.mean(preds == mnist.train.labels), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy 0.979\n"
     ]
    }
   ],
   "source": [
    "preds = mlp.predict(mnist.test.images)\n",
    "print('Testing Accuracy', round(np.mean(preds == mnist.test.labels), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is pretty easy to design and optimize a neural network by tools like sklearn or tensorflow based on backpropagation method, but note, **the cost function must be differentiable in order to get gradients and the metric may not be the same with the cost function**. For example, we can use cross-entropy as our cost function for classification problems, however we often use accuracy as the metrics which is more explicit although not differentiable. \n",
    "\n",
    "Next, we are going to propose a quantum genetic algorithm, which does not rely on differentiable cost functions, you can define any cost functions. In addition, the mechanism of genetic algorithm can make use of distributed computing as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compute feedforward propagation given input and weights, it is quite easy to initialize them. And with different parameters, we can initialize different neural networks.\n",
    "## Qubit\n",
    "A qubit is a pair of $\\alpha=sin(\\theta)$ and $\\beta=cos(\\theta)$, which shows the uncertainties of a state.\n",
    "\n",
    "## Qubit Encoding\n",
    "In a quantum genetic algorithm, each chromosom consists of a series of qubits that forms a quantum registra. For example, suppose we have $m$ parameters, and each parameter can be encoded as a 5-qubit series, consequently the length of the chromosome should be $5m$. \n",
    "\n",
    "For example, we can denote the $ith$ parameter $x_i$:\n",
    "$$\\left(\\begin{array}{ccccc}\n",
    "\t\t\\alpha_{i1} & \\alpha_{i2} & \\alpha_{i3} & \\alpha_{i4} & \\alpha_{i5}\\\\ \n",
    "\t\t\\beta_{i1} & \\beta_{i2} & \\beta_{i3} & \\beta_{i4} & \\beta_{i5}\n",
    "\t\\end{array}\\right)$$\n",
    "    \n",
    " ## Collapsing\n",
    " We can map the qubits series above into states(5 binaries) according to certain criteria(will discuss later), for example we can suppose the collapsed states of the qubits above as:\n",
    " $$s_i = [0 \\ 1 \\ 1\\ 0 \\ 0]$$\n",
    " \n",
    " ## Value Calculating\n",
    " Now that we have the collapsed states of a parameter $x_i$, we can calculate the decimal value of it with a specified range:\n",
    " $$x_i = lowerbound + \\frac {\\sum_{j=1}^5 s_{ij} 2^{j-1}} {2^5-1} * (upperbound- lowerbound)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize weights and biases according to uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qubit_len = 5\n",
    "def initializeWeights(layers):\n",
    "    if type(layers) is not list:\n",
    "        print('Wrong input!')\n",
    "        return None\n",
    "    layers_num = len(layers)\n",
    "    weights = []\n",
    "    for l in range(layers_num-1):\n",
    "        #Generate angles\n",
    "        weight = {}\n",
    "        #Each weight varibale is encoded with 4 qubits\n",
    "        #With the 4 qubits, we can map them into a value\n",
    "        degrees = 2 * math.pi * np.random.uniform(0, 1, size=(layers[l], layers[l+1], qubit_len))\n",
    "        weight['sin'] = np.sin(degrees)\n",
    "        weight['cos'] = np.cos(degrees)\n",
    "        weight['degree'] = degrees\n",
    "        weights.append(weight)\n",
    "    return weights\n",
    "\n",
    "def initializeBiases(layers):\n",
    "    if type(layers) is not list:\n",
    "        print('Wrong input!')\n",
    "        return None\n",
    "    layers_num = len(layers)\n",
    "    biases = []\n",
    "    for l in range(layers_num-1):\n",
    "        bias = {}\n",
    "        #Each bias is encoded in 4 qubits\n",
    "        #With the 4 qubits, we can map them into a value\n",
    "        degrees = 2 * math.pi * np.random.uniform(0, 1, size=(layers[l+1], qubit_len))\n",
    "        bias['sin'] = np.sin(degrees)\n",
    "        bias['cos'] = np.cos(degrees)\n",
    "        bias['degrees'] = degrees\n",
    "        biases.append(bias)\n",
    "    return biases\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers = [784, 128, 10]\n",
    "weights_qubit = initializeWeights(layers)\n",
    "biases_qubit = initializeBiases(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class feedforwardnetwork:\n",
    "    '''\n",
    "    Args:\n",
    "    input_data: input matrix,[batch_size X feature_num], array\n",
    "    input_labels: one-hot labels, array\n",
    "    weights_qubit: define the weights, qubit encoding, list\n",
    "    biases_qubit: define the biases, qubit_encoding, list\n",
    "    '''\n",
    "    def __init__(self, input_data, input_labels, weights_qubit, biases_qubit):\n",
    "        self.input_data = input_data\n",
    "        self.input_labels = input_labels\n",
    "        self.weights_qubit = weights_qubit\n",
    "        self.biases_qubit = biases_qubit\n",
    "        \n",
    "    #Define sigmoid function\n",
    "    def sigmoid(self, Z):\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "    \n",
    "    #Calculate the accuracy\n",
    "    def calAccuracy(self, y, y_test):\n",
    "        '''Calculate Accuracy'''\n",
    "        return np.mean(y==y_test)\n",
    "    \n",
    "    def collapse(self, alpha, beta):\n",
    "        '''\n",
    "        Collapse the quantum state into a binary value\n",
    "        To calculate values later\n",
    "        Args:\n",
    "        alpha: the first state\n",
    "        beta: the second state\n",
    "        '''\n",
    "        pick = np.random.uniform(0, 1, alpha.shape)\n",
    "        #If the random value greater than alpha square, the return true\n",
    "        #Note this can lead to uncertainties for the results\n",
    "        states = np.where(pick > alpha**2, 1, 0)\n",
    "        return states\n",
    "\n",
    "    \n",
    "    def binary2decimal(self, states, bound):\n",
    "        '''\n",
    "        Map binary values of a variable into a decimal value\n",
    "        Each parameter can be mapped into binary bits\n",
    "        Args:\n",
    "        states: binary bits, like [0 1 1 0]\n",
    "        bound: the border of the variable\n",
    "        \n",
    "        Return: variable values\n",
    "        '''\n",
    "        shape = states.shape\n",
    "        #For weights\n",
    "        if len(shape) > 2:\n",
    "            qubit_num = shape[-1]\n",
    "            values = np.zeros((shape[0], shape[1]))\n",
    "            for l in np.arange(qubit_num):\n",
    "                values += states[:, :, l] * (2**l)\n",
    "                values = -bound + values/(2**qubit_len-1)*2*bound\n",
    "        #For biases\n",
    "        else:\n",
    "            qubit_num = shape[-1]\n",
    "            values = np.zeros((shape[0]))\n",
    "            for l in np.arange(qubit_num):\n",
    "                values += states[:, l] * (2**l)\n",
    "                values = -bound + values/(2**qubit_len-1)*2*bound\n",
    "        return values\n",
    "    \n",
    "    def quantum2value(self):\n",
    "        '''\n",
    "        Map varible qubits into decimal values\n",
    "        '''\n",
    "        self.weights = []\n",
    "        self.weights_bits = []\n",
    "        for weight_qubit in self.weights_qubit:\n",
    "            states = self.collapse(weight_qubit['sin'], weight_qubit['cos'])\n",
    "            values = self.binary2decimal(states, 0.01)\n",
    "            self.weights.append(values)\n",
    "            self.weights_bits.append(states)\n",
    "        self.biases = []\n",
    "        self.biases_bits = []\n",
    "        for bias_qubit in self.biases_qubit:\n",
    "            states = self.collapse(bias_qubit['sin'], bias_qubit['cos'])\n",
    "            values = self.binary2decimal(states, 0.01)\n",
    "            self.biases.append(values)\n",
    "            self.biases_bits.append(states)\n",
    "        #return self.weights, self.biases\n",
    "        \n",
    "        \n",
    "    \n",
    "    #Create a function to do predictions\n",
    "    #Map a one-hot vector to a number\n",
    "    def vec2num(self, data, label_num=10):\n",
    "        '''Make predictions'''\n",
    "        if len(data.shape) < 2:\n",
    "            print('The input has too few dimensions')\n",
    "            return None\n",
    "        #Select the class which has largest probability\n",
    "        predictions = [(z.argmax()+ 1)%label_num for z in data]    \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    \n",
    "    def costFuncWithReg(self, h, lambda1=0.01):\n",
    "        '''\n",
    "        Calculate the cost of neural network\n",
    "        Note here we use cross entropy\n",
    "        Regularization is also taken into account\n",
    "        '''\n",
    "        y = self.input_labels\n",
    "        if h is None or y is None or self.weights is None:\n",
    "            print('Invalid Input!')\n",
    "            return None\n",
    "        sample_num = len(y)#Length of y\n",
    "        #Cost of errors\n",
    "        total = -np.mean(np.sum(y*np.log(h), axis=1))\n",
    "        #Cost of regularization\n",
    "        weights = np.array(self.weights)\n",
    "        reg = 0\n",
    "        for wgt in weights:\n",
    "            reg += np.sum(wgt**2) * lambda1/2/sample_num\n",
    "        total +=  reg    \n",
    "        return total \n",
    "    \n",
    "    def proceed(self):\n",
    "        '''\n",
    "        Finish the procedure of feed forward network \n",
    "        and calculate the output\n",
    "        '''\n",
    "        self.quantum2value()\n",
    "        h, output_h, input_z = self.feedforwardNeuralNetwork()\n",
    "        labels = self.vec2num(self.input_labels)\n",
    "        predictions = self.vec2num(h)\n",
    "        #self.fitness = self.costFuncWithReg(h)\n",
    "        self.accuracy = self.calAccuracy(labels, predictions)\n",
    "        self.fitness = self.accuracy\n",
    "        \n",
    "    def update(self, new_weights_qubit, new_biases_qubit):\n",
    "        '''\n",
    "        Update weights and biases\n",
    "        '''\n",
    "        self.weights_qubit = new_weights_qubit\n",
    "        self.biases_qubit = new_biases_qubit\n",
    "    \n",
    "    #weights: weights for each layer, a list\n",
    "    def feedforwardNeuralNetwork(self):\n",
    "        '''Calculate feedforward propagation output'''\n",
    "        ######Deal with extreme cases###\n",
    "        X = self.input_data\n",
    "        if X is None or self.weights is None:\n",
    "            print('Invalid Input!')\n",
    "            return None\n",
    "        dim = X.shape\n",
    "        if len(dim) < 2:\n",
    "            print('X has too less variables')\n",
    "            return None\n",
    "        #####Define variables###########\n",
    "        layer_num = len(self.weights)\n",
    "        output_h = []#Output for each layer\n",
    "        output_h.append(X)#The first layer is equal to input X\n",
    "        input_z = []#Input for each layer, starts from the second layer\n",
    "        #####Make alculations for each layer, except the input layer\n",
    "        for i in range(layer_num):\n",
    "            z = np.dot(output_h[i], self.weights[i])\n",
    "            z += self.biases[i]\n",
    "            h = self.sigmoid(z)\n",
    "            output_h.append(h)\n",
    "            input_z.append(z)\n",
    "        return h, output_h, input_z    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_images, batch_labels = mnist.train.images, mnist.train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fw = feedforwardnetwork(batch_images, batch_labels, weights_qubit, biases_qubit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing:1.898\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "fw.proceed()\n",
    "end = time()\n",
    "print('Timing:{:.3f}'.format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes much time to finish computation for each individual. So it must be time-consuming if we have a large population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.090672727272727266"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw.fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fw.update(weights_qubit, biases_qubit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fw.proceed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10250909090909091"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw.fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an feedforward object, we can create an instance of neural network by specifying the parameters. We can also create a population of neural networks with different parameters. How to generate the parameters, that's a question we will answer by genetic algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Create population\n",
    "\n",
    "In this project, we can view the parameters of neural networks as genes, our goal is to find optimal genes, namely   parameters that make those neural networks performance well on predictions.\n",
    "There are four major steps in a genetic algorithm:\n",
    "- Create Population, randomly create parameters and generate neural networks according to the parameters\n",
    "- Calculate fitness, calculate the fitness of each individual neural network\n",
    "- Quantum rotating gates, compared to conventional genetic algorithm, QGA uses quantum rotating gates to update chromosomes.\n",
    "- Mutate, select certain individual neural networks and make mutations on their chromosomes in terms of NOT gating.\n",
    "\n",
    "For more information you can read the paper The Improvement of Quantum Genetic Algorithm and Its Application on Function Optimization, written by Huaixiao Wang, Jianyong Liu and etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create a group of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class initPopulation:\n",
    "    def __init__(self, layers, pop_size=200):\n",
    "        '''\n",
    "        Args:\n",
    "        pop_size: number of population, integer\n",
    "        layers: neuron number for each layer, list\n",
    "        '''\n",
    "        self.pop_size = pop_size\n",
    "        self.layers = layers\n",
    "        \n",
    "    def __initializeWeights__(self):\n",
    "        '''\n",
    "        Initialize weights between each two neigbouring layers\n",
    "        '''\n",
    "        layers = self.layers\n",
    "        if type(layers) is not list:\n",
    "            print('Wrong input!')\n",
    "            return None\n",
    "        layers_num = len(layers)\n",
    "        weights = []\n",
    "        for l in range(layers_num-1):\n",
    "            #Generate angles\n",
    "            weight = {}\n",
    "            #Each weight varibale is encoded with 4 qubits\n",
    "            #With the 4 qubits, we can map them into a value\n",
    "            degrees = 2 * math.pi * np.random.uniform(0, 1, size=(layers[l], layers[l+1], qubit_len))\n",
    "            degrees = np.ones((layers[l], layers[l+1], qubit_len)) * math.pi/4\n",
    "            weight['sin'] = np.sin(degrees)\n",
    "            weight['cos'] = np.cos(degrees)\n",
    "            weight['degree'] = degrees\n",
    "            weights.append(weight)\n",
    "        return weights\n",
    "\n",
    "    def __initializeBiases__(self):\n",
    "        '''\n",
    "        Initialize biases for each layer(except for input layer)\n",
    "        '''\n",
    "        layers = self.layers\n",
    "        if type(layers) is not list:\n",
    "            print('Wrong input!')\n",
    "            return None\n",
    "        layers_num = len(layers)\n",
    "        biases = []\n",
    "        for l in range(layers_num-1):\n",
    "            bias = {}\n",
    "            #Each bias is encoded in 4 qubits\n",
    "            #With the 4 qubits, we can map them into a value\n",
    "            degrees = 2 * math.pi * np.random.uniform(0, 1, size=(layers[l+1], qubit_len))\n",
    "            degrees = np.ones((layers[l+1], qubit_len)) * math.pi/4\n",
    "            bias['sin'] = np.sin(degrees)\n",
    "            bias['cos'] = np.cos(degrees)\n",
    "            bias['degree'] = degrees\n",
    "            biases.append(bias)\n",
    "        return biases\n",
    "    \n",
    "    def generatePop(self):\n",
    "        '''\n",
    "        Generate a group of weights at random\n",
    "        '''\n",
    "        population = []\n",
    "        for i in range(self.pop_size):\n",
    "            weights = self.__initializeWeights__()\n",
    "            biases = self.__initializeBiases__()\n",
    "            #Initialize an individual\n",
    "            individual = feedforwardnetwork(batch_images, batch_labels, weights, biases)\n",
    "            #Calculate fitness\n",
    "            individual.proceed()\n",
    "            population.append(individual)\n",
    "        return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pop = initPopulation(layers)\n",
    "population = pop.generatePop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3.Genetic Algorithm\n",
    "\n",
    "In this part, we are going to realize quantum rotation gates, mutation. \n",
    "## Quantum Rotation Gates\n",
    "\n",
    "Quantum rotation gates is a method to change the probabilities amplitube by shifting the angles which updates the states of parameters, and eventually uodates the values of parameters. We can show the process in a formula below.\n",
    "Note, we introduce the encoding procedurings above, $x_i$ can be encoded in a qubit string with specific lengths:\n",
    "$$\\left(\\begin{array}{ccccc}\n",
    "\t\t\\alpha_{i1} & \\alpha_{i2} & \\alpha_{i3} & \\alpha_{i4} & \\alpha_{i5}\\\\ \n",
    "\t\t\\beta_{i1} & \\beta_{i2} & \\beta_{i3} & \\beta_{i4} & \\beta_{i5}\n",
    "\t\\end{array}\\right)$$\n",
    "    \n",
    " Suppose the $ith$ original qubit of parameter $x_i$ is $\\alpha_{ij}$ and $\\beta_{ij}$,\n",
    " $$\\alpha_{ij} = sin(\\theta_{ij}), \\ \\beta_{ij} = cos(\\theta_{ij})$$\n",
    " \n",
    " And the shift angle is $\\Delta \\theta_{ij}$, therefore the updated qubit of parameter $x_i$:\n",
    " $$\\hat \\alpha_{ij} = sin(\\theta_{ij}-\\Delta \\theta_{ij}) = sin(\\theta_{ij})cos(\\Delta \\theta_{ij})-cos(\\theta_{ij})sin(\\Delta \\theta_{ij})$$\n",
    "  $$\\hat \\beta_{ij} = cos(\\theta_{ij}-\\Delta \\theta_{ij}) = cos(\\theta_{ij})cos(\\Delta \\theta_{ij})+sin(\\theta_{ij})sin(\\Delta \\theta_{ij})$$\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can transform the formula into a matrix operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\left[\\begin{array}{ccccc}\n",
    "\t\t\\hat \\alpha_{ij} \\\\ \n",
    "\t\t\\hat \\beta_{ij}\n",
    "\t\\end{array}\\right] = \\left[\\begin{array}{ccccc}\n",
    "\t\tsin(\\Delta \\theta_{ij}) & -cos(\\Delta \\theta_{ij})\\\\ \n",
    "\t\tcos(\\Delta \\theta_{ij}) & sin(\\Delta \\theta_{ij}) \n",
    "\t\\end{array}\\right] \\left[\\begin{array}{ccccc}\n",
    "\t\t\\alpha_{ij} \\\\ \n",
    "\t\t\\beta_{ij}\n",
    "\t\\end{array}\\right]$$\n",
    "\n",
    "There are some strategies for rotation gates, here we adopt the one in Huaixiao Wang's paper as below:\n",
    "![rotation strategy](rotationGateStrategy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import  copy\n",
    "class GA:\n",
    "    def __init__(self, population):\n",
    "        '''\n",
    "        Initialize genetic algorithm\n",
    "        '''\n",
    "        self.population = population\n",
    "        self.pop_size = len(population)\n",
    "        \n",
    "    def select(self, ratio=0.3):\n",
    "        '''\n",
    "        Randomly select part of original population\n",
    "        '''\n",
    "        #Get the fitness for each individual\n",
    "        fitnesses = [one.fitness for one in iter(self.population)]\n",
    "        fitnesses.insert(0, 0)\n",
    "        #Calculate the sum of the fitness\n",
    "        total_fitness = np.sum(fitnesses)\n",
    "        #Normalization\n",
    "        fitnesses = np.array(fitnesses)/total_fitness\n",
    "        #Accumulated sum\n",
    "        probs = np.cumsum(fitnesses)\n",
    "        select_num = int(ratio * len(self.population))\n",
    "        select_pop_index = []\n",
    "        #Select a parent according to its fitness value\n",
    "        for _ in np.arange(select_num):\n",
    "            p = np.random.uniform(0, 1)\n",
    "            for i in np.arange(len(probs)-1):\n",
    "                if (p<=probs[i+1]) & (p>probs[i]):\n",
    "                    select_pop_index.append(i)\n",
    "                    break\n",
    "        return select_pop_index\n",
    "    \n",
    "    def rotationMatrix(self, sgn, delta):\n",
    "        '''\n",
    "        Calculation the matrix of rotation gate\n",
    "        Args:\n",
    "        sgn: sign of the angle rotation direction, +1 or -1\n",
    "        delta: shift angle of the rotation gate\n",
    "        '''\n",
    "        e = sgn *delta\n",
    "        U = np.array([[np.cos(e), -np.sin(e)], [np.sin(e), np.cos(e)]])\n",
    "        return U\n",
    "    \n",
    "    def rotationAngleDirection(self, bestIndividual, obj):\n",
    "        '''\n",
    "        Calculate rotation angles and directions for each individual\n",
    "        '''\n",
    "        #Initialize the shift angle\n",
    "        delta_theta = 0.01 * math.pi\n",
    "        #Compare the fitness\n",
    "        fitness_flag = obj.fitness > bestIndividual.fitness\n",
    "        #Traverse each weight layer\n",
    "        for j in np.arange(len(obj.weights_qubit)):\n",
    "            #The jth layer\n",
    "            #Traverse each parameter in this layer\n",
    "            #qubit contains alpha and beta\n",
    "            #Alpha size: layer(i), layer(i+1), bit length\n",
    "            #A qubit is a pair of alpha and beta\n",
    "            weight_alpha = obj.weights_qubit[j]['sin']\n",
    "            #beta size: layer(i), layer(i+1), bit length\n",
    "            weight_beta = obj.weights_qubit[j]['cos']\n",
    "            #Original angles:\n",
    "            degrees = obj.weights_qubit[j]['degree']\n",
    "            #state size: layer(i), layer(i+1), bit length\n",
    "            #state, e.g [0 1 1 0]\n",
    "            weight_bit = obj.weights_bits[j]\n",
    "            #Traverse each parameter\n",
    "            best_weight_bit = bestIndividual.weights_bits[j]\n",
    "            #Search the table\n",
    "            criteria = (weight_bit + best_weight_bit) == 1\n",
    "            #Calculate shift angles for each parameter\n",
    "            delta = criteria * delta_theta\n",
    "            #Calculate the sign of shift angles\n",
    "            sgns = np.zeros(weight_bit.shape)#Initialize it with zeros\n",
    "            #Try to avoid loops, use matrix operation as much as mossible\n",
    "            #IF xi=0 besti=1, then the difference will be -1\n",
    "            current_best_bit_flag = weight_bit - best_weight_bit\n",
    "            #Create a matrix of fitness flag with the same shape as the weight\n",
    "            fitness_flags = np.ones(weight_bit.shape) * fitness_flag\n",
    "            #Map 0 into -1\n",
    "            fitness_flags = np.where(fitness_flags>0, 1, -1)\n",
    "            alpha_beta_pos = (weight_alpha * weight_beta) > 0\n",
    "            alpha_beta_neg = (weight_alpha * weight_beta) < 0\n",
    "            alpha_zero = weight_alpha == 0\n",
    "            beta_zero = weight_beta == 0\n",
    "            #if alpha * beta>0\n",
    "            sgns += current_best_bit_flag * fitness_flags * alpha_beta_pos\n",
    "            #if alpha * beta<0\n",
    "            sgns += (-1)*current_best_bit_flag * fitness_flags * alpha_beta_neg\n",
    "            #if alpha = 0\n",
    "            #Gnerate +1 -1 at random\n",
    "            direction = np.random.choice([1, -1], size=weight_bit.shape)\n",
    "            criteria = current_best_bit_flag * fitness_flags * alpha_zero < 0\n",
    "            sgns += criteria * direction\n",
    "            #if beta = 0\n",
    "            criteria = current_best_bit_flag * fitness_flags * beta_zero > 0\n",
    "            sgns += criteria * direction\n",
    "            #Calculate shift angles\n",
    "            angles = delta * sgns\n",
    "            #Calculate new angles\n",
    "            degrees = degrees - angles\n",
    "            obj.weights_qubit[j]['sin'] = np.sin(degrees)\n",
    "            obj.weights_qubit[j]['cos'] = np.cos(degrees)\n",
    "            obj.weights_qubit[j]['degree'] = degrees\n",
    "        #Update bias\n",
    "        for j in np.arange(len(obj.biases_qubit)):\n",
    "            #The jth bias\n",
    "            #Traverse each parameter in this layer\n",
    "            #qubit contains alpha and beta\n",
    "            #Alpha size: layer(i) bit length\n",
    "            #A qubit is a pair of alpha and beta\n",
    "            bias_alpha = obj.biases_qubit[j]['sin']\n",
    "            #beta size: layer(i), layer(i+1), bit length\n",
    "            bias_beta = obj.biases_qubit[j]['cos']\n",
    "            #Original angles:\n",
    "            degrees = obj.biases_qubit[j]['degree']\n",
    "            #state size: layer(i), layer(i+1), bit length\n",
    "            #state, e.g [0 1 1 0]\n",
    "            bias_bit = obj.biases_bits[j]\n",
    "            #Traverse each parameter\n",
    "            best_bias_bit = bestIndividual.biases_bits[j]\n",
    "            #Search the table\n",
    "            criteria = (bias_bit + best_bias_bit) == 1\n",
    "            #Calculate shift angles for each parameter\n",
    "            delta = criteria * delta_theta\n",
    "            #Calculate the sign of shift angles\n",
    "            #Shape: bias number * bit length\n",
    "            sgns = np.zeros(bias_bit.shape)#Initialize it with zeros\n",
    "            #Try to avoid loops, use matrix operation as much as mossible\n",
    "            #IF xi=0 besti=1, then the difference will be -1\n",
    "            current_best_bit_flag = bias_bit - best_bias_bit\n",
    "            #Create a matrix of fitness flag with the same shape as the weight\n",
    "            fitness_flags = np.ones(bias_bit.shape) * fitness_flag\n",
    "            #Map 0 into -1\n",
    "            fitness_flags = np.where(fitness_flags>0, 1, -1)\n",
    "            alpha_beta_pos = (bias_alpha * bias_beta) > 0\n",
    "            alpha_beta_neg = (bias_alpha * bias_beta) < 0\n",
    "            alpha_zero = bias_alpha == 0\n",
    "            beta_zero = bias_beta == 0\n",
    "            #if alpha * beta>0\n",
    "            sgns += current_best_bit_flag * fitness_flags * alpha_beta_pos\n",
    "            #if alpha * beta<0\n",
    "            sgns += (-1)*current_best_bit_flag * fitness_flags * alpha_beta_neg\n",
    "            #if alpha = 0\n",
    "            #Gnerate +1 -1 at random\n",
    "            direction = np.random.choice([1, -1], size=bias_bit.shape)\n",
    "            criteria = current_best_bit_flag * fitness_flags * alpha_zero < 0\n",
    "            sgns += criteria * direction\n",
    "            #if beta = 0\n",
    "            criteria = current_best_bit_flag * fitness_flags * beta_zero > 0\n",
    "            sgns += criteria * direction\n",
    "            #Calculate shift angles\n",
    "            angles = delta * sgns\n",
    "            #Calculate new angles\n",
    "            degrees = degrees - angles\n",
    "            obj.biases_qubit[j]['sin'] = np.sin(degrees)\n",
    "            obj.biases_qubit[j]['cos'] = np.cos(degrees)\n",
    "            obj.biases_qubit[j]['degree'] = degrees\n",
    "        obj.proceed()\n",
    "        return obj\n",
    "    \n",
    "    def rotatingGates(self, bestIndividual_index):\n",
    "        '''\n",
    "        Rotate gates of quantum registra,\n",
    "        Note, we try to make use of numpy's matrix operations to speed\n",
    "        Computation\n",
    "        Args:\n",
    "        bestIndividual_index: the index of the best individual\n",
    "        '''\n",
    "        bestIndividual = self.population[bestIndividual_index]\n",
    "        #Traverse each individual\n",
    "        for i in np.arange(self.pop_size):\n",
    "            obj = self.population[i]\n",
    "            obj = self.rotationAngleDirection(bestIndividual, obj)\n",
    "            self.population[i] = obj\n",
    "            \n",
    "    def NotGates(self, ratio=0.1):\n",
    "        '''\n",
    "        Rotate gates of quantum registra,\n",
    "        Note, we try to make use of numpy's matrix operations to speed\n",
    "        Computation\n",
    "        Args:\n",
    "        bestIndividual_index: the index of the best individual\n",
    "        '''\n",
    "        #Traverse each individual\n",
    "        num = int(self.pop_size * ratio)\n",
    "        indice = np.random.choice(self.pop_size, num)\n",
    "        for i in indice:\n",
    "            obj = self.population[i]\n",
    "            obj = self.mutation(obj)\n",
    "            self.population[i] = obj\n",
    "    \n",
    "    def mutation(self, obj):\n",
    "        '''\n",
    "        Mutation at several random point within an individual\n",
    "        '''\n",
    "        #Traverse each weight layer\n",
    "        for j in np.arange(len(obj.weights_qubit)):\n",
    "            #The jth layer\n",
    "            #Traverse each parameter in this layer\n",
    "            #qubit contains alpha and beta\n",
    "            #Alpha size: layer(i), layer(i+1), bit length\n",
    "            #A qubit is a pair of alpha and beta\n",
    "            weight_alpha = obj.weights_qubit[j]['sin']\n",
    "            #beta size: layer(i), layer(i+1), bit length\n",
    "            weight_beta = obj.weights_qubit[j]['cos']\n",
    "            #Degrees\n",
    "            degrees = obj.weights_qubit[j]['degree']\n",
    "            picks = np.random.uniform(0, 1, size=degrees.shape)\n",
    "            #state size: layer(i), layer(i+1), bit length\n",
    "            alpha_flag = weight_alpha < picks\n",
    "            beta_flag = weight_beta < picks\n",
    "            degrees = degrees - alpha_flag*beta_flag*math.pi/2\n",
    "            obj.weights_qubit[j]['sin'] = np.sin(degrees)\n",
    "            obj.weights_qubit[j]['cos'] = np.cos(degrees)\n",
    "            obj.weights_qubit[j]['degree'] = degrees\n",
    "        #Update bias\n",
    "        for j in np.arange(len(obj.biases_qubit)):\n",
    "            #The jth bias\n",
    "            #Traverse each parameter in this layer\n",
    "            #qubit contains alpha and beta\n",
    "            #Alpha size: layer(i) bit length\n",
    "            #A qubit is a pair of alpha and beta\n",
    "            bias_alpha = obj.biases_qubit[j]['sin']\n",
    "            #beta size: layer(i), layer(i+1), bit length\n",
    "            bias_beta = obj.biases_qubit[j]['cos']\n",
    "            #Original angles:\n",
    "            degrees = obj.biases_qubit[j]['degree']\n",
    "            picks = np.random.uniform(0, 1, size=degrees.shape)\n",
    "            alpha_flag = bias_alpha < picks\n",
    "            beta_flag = bias_beta < picks\n",
    "            degrees = degrees - alpha_flag*beta_flag*math.pi/2\n",
    "            #Calculate new angles\n",
    "            degrees = degrees - alpha_flag*beta_flag*math.pi/2\n",
    "            obj.biases_qubit[j]['sin'] = np.sin(degrees)\n",
    "            obj.biases_qubit[j]['cos'] = np.cos(degrees)\n",
    "            obj.biases_qubit[j]['degree'] = degrees\n",
    "        obj.proceed()\n",
    "        return obj\n",
    "        \n",
    "    def proceed(self, generation_num = 1):\n",
    "        '''\n",
    "        Execute quantum rotation and not gating\n",
    "        '''\n",
    "        #Keep the best individual\n",
    "        best_fitness, optimal_index = self.findMaximalIndividual()\n",
    "        best_individual = self.population[optimal_index]\n",
    "        for _ in np.arange(generation_num):\n",
    "            #Quantum Rotation Gate\n",
    "            print('Best Fitness:', round(best_fitness, 4))\n",
    "            self.rotatingGates(optimal_index)\n",
    "            #Mutation\n",
    "            self.NotGates()\n",
    "            fitness, index = ga.findMaximalIndividual()\n",
    "            if fitness < best_fitness:\n",
    "                self.population[index] = best_individual\n",
    "            else:\n",
    "                best_fitness = fitness\n",
    "                optimal_index = index\n",
    "                best_individual = self.population[optimal_index]\n",
    "        #return self.population\n",
    "        \n",
    "    def findMaximalIndividual(self):\n",
    "        accuracies = np.array([one.accuracy for one in self.population])\n",
    "        fitnesses = np.array([one.fitness for one in self.population])\n",
    "        optimal_value = max(fitnesses)\n",
    "        optimal_index = fitnesses.argmax()\n",
    "        return optimal_value, optimal_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ga = GA(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Fitness: 0.1541\n",
      "Best Fitness: 0.1541\n",
      "Best Fitness: 0.1541\n",
      "Best Fitness: 0.1643\n",
      "Best Fitness: 0.1674\n",
      "Best Fitness: 0.1674\n",
      "Best Fitness: 0.1674\n",
      "Best Fitness: 0.1674\n",
      "Best Fitness: 0.1674\n",
      "Best Fitness: 0.1674\n",
      "Best Fitness: 0.1674\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1727\n",
      "Best Fitness: 0.1851\n",
      "Best Fitness: 0.1851\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n",
      "Best Fitness: 0.1865\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-234-fa1cfa74b156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#Try 20 generations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mga\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproceed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-232-6f370e96bccf>\u001b[0m in \u001b[0;36mproceed\u001b[0;34m(self, generation_num)\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[1;31m#Quantum Rotation Gate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best Fitness:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_fitness\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotatingGates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimal_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m             \u001b[1;31m#Mutation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotGates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-232-6f370e96bccf>\u001b[0m in \u001b[0;36mrotatingGates\u001b[0;34m(self, bestIndividual_index)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotationAngleDirection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbestIndividual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-232-6f370e96bccf>\u001b[0m in \u001b[0;36mrotationAngleDirection\u001b[0;34m(self, bestIndividual, obj)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases_qubit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cos'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases_qubit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'degree'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproceed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-214-036635c2982f>\u001b[0m in \u001b[0;36mproceed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m         '''\n\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantum2value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeedforwardNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvec2num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvec2num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-214-036635c2982f>\u001b[0m in \u001b[0;36mfeedforwardNeuralNetwork\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[1;31m#####Make alculations for each layer, except the input layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_h\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mz\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Try 20 generations\n",
    "ga.proceed(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
